{
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python", 
            "name": "python2-spark20"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown", 
            "source": "# Assignment 4\n\nWelcome to Assignment 4. This will be the most fun. Now we will prepare data for plotting.\n\nJust make sure you hit the play button on each cell from top to down. There are three functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook. Please also make sure to only implement the function bodies and DON'T add any additional code outside functions since this might confuse the autograder.\n\nSo the function below is used to make it easy for you to create a data frame from a cloudant data frame using the so called \"DataSource\" which is some sort of a plugin which allows ApacheSpark to use different data sources.\n"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 11, 
            "source": "#Please don't modify this function\ndef readDataFrameFromCloudant(host,user,pw,database):\n    cloudantdata=spark.read.format(\"com.cloudant.spark\"). \\\n    option(\"cloudant.host\",host). \\\n    option(\"cloudant.username\", user). \\\n    option(\"cloudant.password\", pw). \\\n    load(database)\n\n    cloudantdata.createOrReplaceTempView(\"washing\")\n    spark.sql(\"SELECT * from washing\").show()\n    return cloudantdata"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Sampling is one of the most important things when it comes to visualization because often the data set get so huge that you simply\n\n- can't copy all data to a local Spark driver (Data Science Experience is using a \"local\" Spark driver)\n- can't throw all data at the plotting library\n\nPlease implement a function which returns a 10% sample of a given data frame:"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 12, 
            "source": "def getSample(df,spark):\n    \n    result = spark.sql(\"select d.temperature from washing where d.temperature is not null\")\n    \n    mySearchingPiece = int((result.count()*10)/100)\n    \n    # result_array = result.rdd.map(lambda row : row.temperature).sample(False,0.1)\n    result_array = result.rdd.map(lambda row : row.temperature).sample(False,mySearchingPiece)\n    return result_array"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now we want to create a histogram and boxplot. Please ignore the sampling for now and retur a python list containing all temperature values from the data set"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 13, 
            "source": "def getListForHistogramAndBoxPlot(df,spark):\n    \n    result = spark.sql(\"select d.temperature from washing where d.temperature is not null\")\n    \n    mySearchingPiece = int((result.count()*10)/100)\n    \n    # result_array = result.rdd.map(lambda row : row.temperature).sample(False,0.1)\n    result_array = result.rdd.map(lambda row : row.temperature).sample(False,mySearchingPiece).collect()\n           \n    return result_array"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Finally we want to create a run chart. Please return two lists (encapusalted in a python tuple object) containing temperature and timestamp (ts) ordered by timestamp. Please refere to the following link to learn more about tuples in python: https://www.tutorialspoint.com/python/python_tuples.htm"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 14, 
            "source": "#should return a tuple containing the two lists for timestamp and temperature\n#please make sure you take only 10% of the data by sampling\n#please also ensure that you sample in a way that the timestamp samples and temperature samples correspond (=> call sample on an object still containing both dimensions)\ndef getListsForRunChart(df,spark):\n    \n    result = spark.sql(\"select d.temperature,d.ts from washing where d.temperature is not null order by d.ts asc\")\n    \n    mySearchingPiece = int((result.count()*10)/100)\n    \n    # result_rdd = result.rdd.sample(False,0.1).map(lambda row : (row.ts,row.temperature))\n    result_rdd = result.rdd.sample(False,mySearchingPiece).map(lambda row : (row.ts,row.temperature))\n    \n    result_array_ts = result_rdd.map(lambda (ts,temperature): ts).collect()\n    result_array_temperature = result_rdd.map(lambda (ts,temperature): temperature).collect()\n    return (result_array_ts,result_array_temperature)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED\n#axx\n### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+--------------------+--------------------+--------------------+\n|                 _id|                _rev|                   d|\n+--------------------+--------------------+--------------------+\n|0f83931363c7ce12d...|1-ffedf0982164b41...|[2,null,null,74,n...|\n|0f83931363c7ce12d...|1-25fb1d3ec6eecb7...|[2,null,null,null...|\n|0f83931363c7ce12d...|1-69c8b2d638db12e...|[10,11,acceptable...|\n|0f83931363c7ce12d...|1-2e2a41b45fcb93a...|[11,11,acceptable...|\n|0f83931363c7ce12d...|1-7f45175964250f8...|[5,null,null,68,n...|\n|0f83931363c7ce12d...|1-75f7f3cdb9a3787...|[17,11,acceptable...|\n|0f83931363c7ce12d...|1-a53fb4cd7965f0f...|[20,11,acceptable...|\n|0f83931363c7ce12d...|1-2507d6eb21e3936...|[34,11,acceptable...|\n|0f83931363c7ce12d...|1-fc51c2dbb04a718...|[43,11,acceptable...|\n|0f83931363c7ce12d...|1-91dd8f0ce02523b...|[50,11,acceptable...|\n|0f83931363c7ce12d...|1-3685ecced4c3b9f...|[51,11,acceptable...|\n|0f83931363c7ce12d...|1-71d10c74297acf1...|[55,11,acceptable...|\n|0f83931363c7ce12d...|1-c9520469ec2a58e...|[72,11,acceptable...|\n|0f83931363c7ce12d...|1-5d5d94caffcd155...|[85,11,acceptable...|\n|0f83931363c7ce12d...|1-5d5b472676a01f2...|[102,11,acceptabl...|\n|0f83931363c7ce12d...|1-3a04267e59f13d6...|[107,11,acceptabl...|\n|0f83931363c7ce12d...|1-1139a93187213fd...|[108,11,acceptabl...|\n|0f83931363c7ce12d...|1-08a6b268169f1cb...|[120,11,acceptabl...|\n|0f83931363c7ce12d...|1-34bad88df7dfd0e...|[124,11,acceptabl...|\n|0f83931363c7ce12d...|1-b3845fe45177cfb...|[44,null,null,79,...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 15, 
            "source": "#TODO Please provide your Cloudant credentials here\nhostname = \"0f8410a3-0e0f-467f-a3cf-b9fbd5da37c2-bluemix.cloudant.com\"\nuser = \"0f8410a3-0e0f-467f-a3cf-b9fbd5da37c2-bluemix\"\npw = \"1363e31faf953220b28f78a9afae906603f2882db47b0e9375bf650569f4d368\"\ndatabase = \"washing\"\ncloudantdata=readDataFrameFromCloudant(hostname, user, pw, database)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 16, 
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mAnalysisException\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-17-8b99a8300b5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetListForHistogramAndBoxPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloudantdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m<ipython-input-13-64877c2379db>\u001b[0m in \u001b[0;36mgetListForHistogramAndBoxPlot\u001b[1;34m(df, spark)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetListForHistogramAndBoxPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select temperature from washing where temperature is not null\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmySearchingPiece\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m         \"\"\"\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark20master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mAnalysisException\u001b[0m: u\"cannot resolve '`temperature`' given input columns: [_id, _rev, d]; line 1 pos 38;\\n'Project ['temperature]\\n+- 'Filter isnotnull('temperature)\\n   +- SubqueryAlias washing\\n      +- Relation[_id#7,_rev#8,d#9] CloudantReadWriteRelation(com.cloudant.spark.CloudantConfig@fcfb6009,StructType(StructField(_id,StringType,true), StructField(_rev,StringType,true), StructField(d,StructType(StructField(count,LongType,true), StructField(flowrate,LongType,true), StructField(fluidlevel,StringType,true), StructField(frequency,LongType,true), StructField(hardness,LongType,true), StructField(speed,LongType,true), StructField(temperature,LongType,true), StructField(ts,LongType,true), StructField(voltage,LongType,true)),true)),[_id: string, _rev: string ... 1 more field])\\n\""
                    ], 
                    "ename": "AnalysisException", 
                    "evalue": "u\"cannot resolve '`temperature`' given input columns: [_id, _rev, d]; line 1 pos 38;\\n'Project ['temperature]\\n+- 'Filter isnotnull('temperature)\\n   +- SubqueryAlias washing\\n      +- Relation[_id#7,_rev#8,d#9] CloudantReadWriteRelation(com.cloudant.spark.CloudantConfig@fcfb6009,StructType(StructField(_id,StringType,true), StructField(_rev,StringType,true), StructField(d,StructType(StructField(count,LongType,true), StructField(flowrate,LongType,true), StructField(fluidlevel,StringType,true), StructField(frequency,LongType,true), StructField(hardness,LongType,true), StructField(speed,LongType,true), StructField(temperature,LongType,true), StructField(ts,LongType,true), StructField(voltage,LongType,true)),true)),[_id: string, _rev: string ... 1 more field])\\n\""
                }
            ], 
            "cell_type": "code", 
            "execution_count": 17, 
            "source": "plt.hist(getListForHistogramAndBoxPlot(cloudantdata,spark))\nplt.show()"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "plt.boxplot(getListForHistogramAndBoxPlot(cloudantdata,spark))\nplt.show()"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "lists = getListsForRunChart(cloudantdata,spark)"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "plt.plot(lists[0],lists[1])\nplt.xlabel(\"time\")\nplt.ylabel(\"temperature\")\nplt.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Congratulations, you are done! Please download the notebook as python file, name it assignment4.1.py and sumbit it to the grader."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 0
}