{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "name": "python2-spark20", 
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "source": "# Assignment 3\n\nWelcome to Assignment 3. This will be even more fun. Now we will calculate statistical measures on the test data you have created.\n\nYOU ARE NOT ALLOWED TO USE ANY OTHER 3RD PARTY LIBRARIES LIKE PANDAS. PLEASE ONLY MODIFY CONTENT INSIDE THE FUNCTION SKELETONS\nPlease read why: https://www.coursera.org/learn/exploring-visualizing-iot-data/discussions/weeks/3/threads/skjCbNgeEeapeQ5W6suLkA\n. Just make sure you hit the play button on each cell from top to down. There are seven functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook.\nPlease also make sure to only implement the function bodies and DON'T add any additional code outside functions since this might confuse the autograder.\n\nSo the function below is used to make it easy for you to create a data frame from a cloudant data frame using the so called \"DataSource\" which is some sort of a plugin which allows ApacheSpark to use different data sources.\n", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "#Please don't modify this function\ndef readDataFrameFromCloudant(host,user,pw,database):\n    cloudantdata=spark.read.format(\"com.cloudant.spark\"). \\\n    option(\"cloudant.host\",host). \\\n    option(\"cloudant.username\", user). \\\n    option(\"cloudant.password\", pw). \\\n    load(database)\n\n    cloudantdata.createOrReplaceTempView(\"washing\")\n    spark.sql(\"SELECT * from washing\").show()\n    return cloudantdata", 
            "execution_count": 597, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "All functions can be implemented using DataFrames, ApacheSparkSQL or RDDs. We are only interested in the result. You are given the reference to the data frame in the \"df\" parameter and in case you want to use SQL just use the \"spark\" parameter which is a reference to the global SparkSession object. Finally if you want to use RDDs just use \"df.rdd\" for obtaining a reference to the underlying RDD object. \n\nLet's start with the first function. Please calculate the minimal temperature for the test data set you have created. We've provided a little skeleton for you in case you want to use SQL. You can use this skeleton for all subsequent functions. Everything can be implemented using SQL only if you like.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "def minTemperature(df,spark):\n    # return spark.sql(\"SELECT * as mintemp from washing\").first().mintemp\n    \n    # result = spark.sql(\"SELECT d.temperature from washing WHERE d.temperature IS NOT NULL\")\n    # return result.show()\n    \n    # df = df.schema\n    # return df\n    \n    return spark.sql(\"SELECT MIN(d.temperature) as minTemp from washing WHERE d.temperature IS NOT NULL\").first().minTemp", 
            "execution_count": 598, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Please now do the same for the mean of the temperature", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def meanTemperature(df,spark):\n    \n    # spark.sql(\"SELECT SUM(d.temperature) as totalTemp from washing\").first().totalTemp\n    # mean = df.sum(\"d.temperature\")/float(df.count(\"d.temperature\"))\n    # return mean.show()\n    # return ##INSERT YOUR CODE HERE##\n    \n    # solucao do exercicio feito por sql\n    sumTemperature = spark.sql(\"SELECT SUM(d.temperature) as sumTemp from washing WHERE d.temperature IS NOT NULL\").first().sumTemp\n    numTemperature = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\").count()\n    mean = sumTemperature/float(numTemperature)\n    # ##################################\n   \n    # solucao do exercicio feito em spark\n    # myList = df.select(\"d.temperature\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    # myList = filter(None, myList)\n    # rdd = sc.parallelize(myList)\n    # mean = rdd.sum()/float(rdd.count())\n    # #####################################\n    \n    return mean", 
            "execution_count": 599, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Please now do the same for the maximum of the temperature", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "def maxTemperature(df,spark):\n    \n    return spark.sql(\"SELECT MAX(d.temperature) as maxtemp from washing WHERE d.temperature IS NOT NULL\").first().maxtemp", 
            "execution_count": 600, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Please now do the same for the standard deviation of the temperature", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def sdTemperature(df,spark):\n    from math import sqrt\n    from pyspark import SparkContext\n    # sc=SparkContext()\n    sc = SparkContext.getOrCreate()\n    \n    # solucao do exercicio feito em spark \n    myList = df.select(\"d.temperature\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    myList = filter(None, myList)\n    rdd = sc.parallelize(myList)\n    n = rdd.count()\n\n    # solucao do exercicio feito por sql\n    sumTemperature = spark.sql(\"SELECT SUM(d.temperature) as sumTemp from washing WHERE d.temperature IS NOT NULL\").first().sumTemp\n    numTemperature = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\").count()\n    mean = sumTemperature/float(numTemperature)\n    # ##################################\n    \n    sd = sqrt(rdd.map(lambda x : pow(x - mean,2)).sum()/n)\n    # ######################################################\n    \n    # solucao do exercicio feito em spark \n    # df = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\")\n    # n = df.count()\n    \n    return sd\n\n\n", 
            "execution_count": 601, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Please now do the same for the skew of the temperature. Since the SQL statement for this is a bit more complicated we've provided a skeleton for you. You have to insert custom code at four position in order to make the function work. Alternatively you can also remove everything and implement if on your own. Note that we are making use of two previously defined functions, so please make sure they are correct. Also note that we are making use of python's string formatting capabilitis where the results of the two function calls to \"meanTemperature\" and \"sdTemperature\" are inserted at the \"%s\" symbols in the SQL string.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def skewTemperature(df,spark):    \n# ###### Original Code to Work with SQL Statement, but I prefer to work with RDD\n#    return spark.sql(\"\"\"\n# SELECT \n#    (\n#        1/##INSERT YOUR CODE HERE##\n#    ) *\n#    SUM (\n#        POWER(##INSERT YOUR CODE HERE##-%s,3)/POWER(%s,3)\n#    )\n#\n# as ##INSERT YOUR CODE HERE## from washing\n#                    \"\"\" %(meanTemperature(df,spark),sdTemperature(df,spark)))##INSERT YOUR CODE HERE##\n# #############################\n    from math import sqrt\n    from pyspark import SparkContext\n    # sc=SparkContext()\n    sc = SparkContext.getOrCreate()\n    \n    myList = df.select(\"d.temperature\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    myList = filter(None, myList)\n    rdd = sc.parallelize(myList)\n    \n    # solucao do exercicio feito por sql\n    sumTemperature = spark.sql(\"SELECT SUM(d.temperature) as sumTemp from washing WHERE d.temperature IS NOT NULL\").first().sumTemp\n    numTemperature = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\").count()\n    mean = sumTemperature/float(numTemperature)\n    # ##################################\n    \n    # solucao do exercicio feito em spark \n    n = rdd.count()\n    sd = sqrt(rdd.map(lambda x : pow(x - mean,2)).sum()/n)\n    \n    n = float(n)\n\n    sk = n/((n-1)*(n-2))*rdd.map(lambda x : pow(x - mean,3)/pow(sd,3)).sum()\n\n    return sk\n", 
            "execution_count": 602, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Kurtosis is the 4th statistical moment, so if you are smart you can make use of the code for skew which is the 3rd statistical moment. Actually only two things are different.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "def kurtosisTemperature(df,spark):   \n    from math import sqrt\n    from pyspark import SparkContext\n    # sc=SparkContext()\n    sc = SparkContext.getOrCreate()\n    \n    myList = df.select(\"d.temperature\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    myList = filter(None, myList)\n    rdd = sc.parallelize(myList)\n    \n    # solucao do exercicio feito por sql\n    sumTemperature = spark.sql(\"SELECT SUM(d.temperature) as sumTemp from washing WHERE d.temperature IS NOT NULL\").first().sumTemp\n    numTemperature = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\").count()\n    mean = sumTemperature/float(numTemperature)\n    # ##################################\n    \n    # solucao do exercicio feito em spark \n    n = rdd.count()\n    sd = sqrt(rdd.map(lambda x : pow(x - mean,2)).sum()/n)\n    \n    n = float(n)\n    kurtosis = rdd.map(lambda x : pow(x - mean,4)/pow(sd,4)).sum()/(1-n)\n    \n    return kurtosis", 
            "execution_count": 603, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "Just a hint. This can be solved easily using SQL as well, but as shown in the lecture also using RDDs.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "def correlationTemperatureHardness(df,spark):\n    from math import sqrt\n    from pyspark import SparkContext\n    # sc=SparkContext()\n    sc = SparkContext.getOrCreate()\n    \n    # result = spark.sql(\"SELECT d.* from washing\")\n    # return result.show()\n\n     # fazendo o calculo do mean e do sd para temperatura\n    sumTemperature = spark.sql(\"SELECT SUM(d.temperature) as sumTemp from washing WHERE d.temperature IS NOT NULL\").first().sumTemp\n    numTemperature = spark.sql(\"SELECT d.temperature as numTemp from washing WHERE d.temperature IS NOT NULL\").count()\n    meanTemp = sumTemperature/float(numTemperature)\n    # ##################################\n    \n    # fazendo o calculo do mean e do sd para hardness\n    sumHard = spark.sql(\"SELECT SUM(d.hardness) as sumHard from washing WHERE d.hardness IS NOT NULL\").first().sumHard\n    numHard = spark.sql(\"SELECT d.hardness as numHard from washing WHERE d.hardness IS NOT NULL\").count()\n    meanHard = sumHard/float(numHard)\n    # ##################################                         \n                             \n       \n    # #################################\n    myListTemp = df.select(\"d.temperature\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    myListTemp = filter(None, myListTemp)\n    rddTemp = sc.parallelize(myListTemp)\n    \n    myListHard = df.select(\"d.hardness\").rdd.flatMap(lambda x: x).collect() # transforma uma column do dataset para uma simples lista \n    myListHard = filter(None, myListHard)\n    rddHard = sc.parallelize(myListHard)\n    # #################################\n    \n    rddTH = rddTemp.zip(rddHard)\n    \n    # #################################\n    \n    covTH = rddTH.map(lambda (x,y) : (x - meanTemp)*(y - meanHard)).sum()/rddTH.count()\n\n    # ######################################################\n        \n    n = rddTH.count()\n    \n    # ######################################################\n    \n    sdTemp = sqrt(rddTemp.map(lambda x : pow(x - meanTemp,2)).sum()/n)\n    sdHard = sqrt(rddHard.map(lambda x : pow(x - meanHard,2)).sum()/n)\n    \n    # ######################################################\n    \n    # calculo de ambos os dados, temperature e hardness\n\n    corrTH = covTH/(sdTemp * sdHard)\n    \n    return corrTH  ", 
            "execution_count": 604, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED\n#axx\n### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "#TODO Please provide your Cloudant credentials here\nhostname = \"0f8410a3-0e0f-467f-a3cf-b9fbd5da37c2-bluemix.cloudant.com\"\nuser = \"0f8410a3-0e0f-467f-a3cf-b9fbd5da37c2-bluemix\"\npw = \"1363e31faf953220b28f78a9afae906603f2882db47b0e9375bf650569f4d368\"\ndatabase = \"washing\"\ncloudantdata=readDataFrameFromCloudant(hostname, user, pw, database)", 
            "execution_count": 605, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "+--------------------+--------------------+--------------------+\n|                 _id|                _rev|                   d|\n+--------------------+--------------------+--------------------+\n|0f83931363c7ce12d...|1-ffedf0982164b41...|[2,null,null,74,n...|\n|0f83931363c7ce12d...|1-25fb1d3ec6eecb7...|[2,null,null,null...|\n|0f83931363c7ce12d...|1-69c8b2d638db12e...|[10,11,acceptable...|\n|0f83931363c7ce12d...|1-2e2a41b45fcb93a...|[11,11,acceptable...|\n|0f83931363c7ce12d...|1-7f45175964250f8...|[5,null,null,68,n...|\n|0f83931363c7ce12d...|1-75f7f3cdb9a3787...|[17,11,acceptable...|\n|0f83931363c7ce12d...|1-a53fb4cd7965f0f...|[20,11,acceptable...|\n|0f83931363c7ce12d...|1-2507d6eb21e3936...|[34,11,acceptable...|\n|0f83931363c7ce12d...|1-fc51c2dbb04a718...|[43,11,acceptable...|\n|0f83931363c7ce12d...|1-91dd8f0ce02523b...|[50,11,acceptable...|\n|0f83931363c7ce12d...|1-3685ecced4c3b9f...|[51,11,acceptable...|\n|0f83931363c7ce12d...|1-71d10c74297acf1...|[55,11,acceptable...|\n|0f83931363c7ce12d...|1-c9520469ec2a58e...|[72,11,acceptable...|\n|0f83931363c7ce12d...|1-5d5d94caffcd155...|[85,11,acceptable...|\n|0f83931363c7ce12d...|1-5d5b472676a01f2...|[102,11,acceptabl...|\n|0f83931363c7ce12d...|1-3a04267e59f13d6...|[107,11,acceptabl...|\n|0f83931363c7ce12d...|1-1139a93187213fd...|[108,11,acceptabl...|\n|0f83931363c7ce12d...|1-08a6b268169f1cb...|[120,11,acceptabl...|\n|0f83931363c7ce12d...|1-34bad88df7dfd0e...|[124,11,acceptabl...|\n|0f83931363c7ce12d...|1-b3845fe45177cfb...|[44,null,null,79,...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "minTemperature(cloudantdata,spark)", 
            "execution_count": 606, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "80"
                    }, 
                    "execution_count": 606
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "meanTemperature(cloudantdata,spark)", 
            "execution_count": 607, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "89.92349726775956"
                    }, 
                    "execution_count": 607
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "maxTemperature(cloudantdata,spark)", 
            "execution_count": 608, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "100"
                    }, 
                    "execution_count": 608
                }
            ]
        }, 
        {
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }, 
            "source": "sdTemperature(cloudantdata,spark)", 
            "execution_count": 609, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "6.088567160194603"
                    }, 
                    "execution_count": 609
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "skewTemperature(cloudantdata,spark)", 
            "execution_count": 610, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0.04162061140856722"
                    }, 
                    "execution_count": 610
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "kurtosisTemperature(cloudantdata,spark)", 
            "execution_count": 611, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "-1.7171003066322552"
                    }, 
                    "execution_count": 611
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "source": "correlationTemperatureHardness(cloudantdata,spark)", 
            "execution_count": 612, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "-0.07156349787356674"
                    }, 
                    "execution_count": 612
                }
            ]
        }, 
        {
            "source": "Congratulations, you are done, please download this notebook as python file using the export function and submit is to the gader using the filename \"assignment3.1.py\"", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}